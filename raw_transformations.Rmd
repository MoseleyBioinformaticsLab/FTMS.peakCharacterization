---
title: "Transform Raw Data"
author: "Robert M Flight"
date: "`r Sys.time()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```


## Purpose

Hunter has an idea about applying both the normalization and possibly M/Z correction
to the `raw` scan level data, and see if we can do something about **noise**
detection in general. See [this issue](https://gitlab.cesb.uky.edu/rmflight/SIRM.FTMS.peakCharacterization/issues/21)
for some discussion.

To do this, we need to do two things:

1. Verify that we can sum / average scans the same way that `xcms` does after
extracting the raw data peaks

2. Be able to apply the normalization and/or m/z corrections to the `raw` data
points.

## Verify Summations

First, lets see if we can do what we need to with the raw data points. Normally,
we extract each scan by scan, and if we want to do something with the averaged
spectrum, we let `xcms` do it for us. But now we want to do everything ourselves,
so we need to verify we can get the same result.

```{r xcms_summation}
library(SIRM.FTMS.peakCharacterization)
library(xcms)
library(cowplot)
library(dplyr)

zip_data <- zip_ms("test_files/bcal_good_UK001N1exoposb.mzML", load_raw = TRUE, load_peak_list = FALSE)

xcms_sum <- as.data.frame(xcms::getSpec(zip_data$raw_ms$raw_data, scanrange = c(1, 30)))
```


For our own summation, we will borrow from the source code of `xcms` itself.

```{r own_summation}
all_scans <- purrr::map_df(seq(1, 30), function(in_scan){
  tmp_frame <- as.data.frame(xcms::getScan(zip_data$raw_ms$raw_data, scan = in_scan))
  tmp_frame$scan <- in_scan
  tmp_frame
})

all_mz <- sort(unique(all_scans$mz))

int_matrix <- matrix(0, nrow = length(all_mz), ncol = length(unique(all_scans$scan)))

for (iscan in unique(all_scans$scan)) {
  sub_data <- all_scans[all_scans$scan %in% iscan, ]
  match_mz <- all_mz %in% sub_data$mz
  int_matrix[match_mz, iscan] <- sub_data$intensity
}

all_int <- rowMeans(int_matrix)

own_sum <- data.frame(mz = all_mz, intensity = all_int)
```

```{r compare_sums}
xcms_sum[is.na(xcms_sum$intensity), "intensity"] <- 0

all.equal(xcms_sum, own_sum)
all.equal(xcms_sum$mz, own_sum$mz)

use_points <- sample(seq(1, nrow(own_sum)), 10000)
plot(xcms_sum$intensity[use_points], own_sum$intensity[use_points])
```

OK, not quite what I expected. Looking at the source code of the `getSpec` method,
there is a call to `approx`, which looks like it does some linear interpolation.
The question is: **is that the correct thing to do?**. Only one way to find out.

## Check Whether Linear Interpolation is Good Idea

To verify if using `approx` makes sense, lets do peak detection on the `xcms_sum`,
and then compare the point intensities to see if `xcms` or our method makes more
sense.

```{r find_peaks_compare}
found_peaks <- pracma::findpeaks(xcms_sum$intensity, nups = 4)
peak_index <- seq(found_peaks[1, 3], found_peaks[1, 4])

test_peak <- xcms_sum[peak_index, ]
test_peak$type <- "xcms"
own_sum$type <- "own"
test_peak <- rbind(test_peak, own_sum[peak_index, ])

ggplot(test_peak, aes(x = mz, y = intensity, color = type)) + geom_point() + geom_line()
```

OK, so based on this plot, it **definitely makes sense** to use `approx` and do
some form of linear interpolation.

But lets also evaluate what are the individual peaks from each scan underneath this thing.

```{r indiv_peaks}
mz_range <- range(test_peak$mz)

test_scans <- filter(all_scans, (mz >= mz_range[1]) & (mz <= mz_range[2]))

ggplot(test_scans, aes(x = mz, y = intensity, color = as.factor(scan))) + geom_point() + geom_line()
```

Now lets look at what `approx` does using our little test peak.

```{r use_approx}
test_mz <- all_mz[(all_mz >= mz_range[1]) & (all_mz <= mz_range[2])]

int_df <- purrr::map_df(unique(test_scans$scan), function(in_scan){
  scan_df <- filter(test_scans, scan %in% in_scan)
  out_int <- approx(x = scan_df$mz, y = scan_df$intensity, xout = test_mz)$y
  data.frame(mz = test_mz, intensity = out_int, scan = in_scan)
})

ggplot(int_df, aes(x = mz, y = intensity, color = as.factor(scan))) + geom_point() + geom_line()
```

So, we definitely need to use `approx` to interpolate the values. This is useful 
to know. We at least need to do this when we try to average the points together.

```{r averave_approx}
int_matrix2 <- matrix(0, nrow = length(test_mz), ncol = length(unique(test_scans$scan)))
colnames(int_matrix2) <- unique(test_scans$scan)

for (iscan in unique(test_scans$scan)) {
  scan_df <- filter(test_scans, scan %in% iscan)
  int_matrix2[, as.character(iscan)] <- approx(x = scan_df$mz, y = scan_df$intensity, xout = test_mz)$y
}
int_matrix2[is.na(int_matrix2)] <- 0

own_sum2 <- data.frame(mz = test_mz, intensity = rowMeans(int_matrix2, na.rm = TRUE))

xcms_sum2 <- as.data.frame(xcms::getSpec(zip_data$raw_ms$raw_data, scanrange = unique(test_scans$scan)))
xcms_sum2 <- filter(xcms_sum2, (mz >= mz_range[1]) & (mz <= mz_range[2]))
xcms_sum2$type <- "xcms"
own_sum2$type <- "own"
test_avg <- rbind(own_sum2, xcms_sum2)

ggplot(test_avg, aes(x = mz, y = intensity, color = type)) + geom_line() + geom_point()
```

Hmmm, still have an issue with difference in intensities, lets try doing it for
the 30 scans and see what happens instead.

```{r own_sum_approx}
int_matrix3 <- matrix(nrow = length(all_mz), ncol = length(unique(all_scans$scan)))

for (iscan in unique(all_scans$scan)) {
  sub_data <- all_scans[all_scans$scan %in% iscan, ]
  int_matrix3[, iscan] <- approx(x = sub_data$mz, y = sub_data$intensity, xout = all_mz)$y
}

all_int3 <- rowMeans(int_matrix3)

own_sum3 <- data.frame(mz = all_mz, intensity = all_int3)

xcms_sum2 <- as.data.frame(xcms::getSpec(zip_data$raw_ms$raw_data, scanrange = c(1, 30)))

all.equal(own_sum3, xcms_sum2)
```

WooHoo! Got the exact same results, working from the extracted scans. And it
sort of makes sense, in that we add data points to each scan that are interpolated
from the full set of points. So, based on this, we should be able to normalize
the scan to scan intensities rather easily and then do a new average.

Lets look at our little test peak again between the two cases.

```{r}
own_sum3$type <- "own"
xcms_sum2$type <- "xcms"

test_2 <- rbind(own_sum3, xcms_sum2)
test_2 <- filter(test_2, (mz >= mz_range[1]) & (mz <= mz_range[2]))

ggplot(test_2, aes(x = mz, y = intensity, color = type)) + geom_point() + geom_line()
```

## New Functions

So now we need a new function that can actually do the normalization on the raw
data, and a function that averages the scans properly.

### Average Scans

So lets take the code we created above, and actually make a function that we
can use out of it.

```{r average_scans}
extract_scans_list <- function(raw_data, mz_range = NULL, scan_range = NULL){
  # this code is a re-implementation of the getSpec method in xcms. This is because
  # there is no way to get the scans and the unique_mz from xcms w/out averaging
  # them at the end.
  selection <- xcms::profRange(raw_data, mzrange = mz_range, scanrange = scan_range)
  scan_indices <- selection$scanidx
  scans <- list(length(scan_indices))
  unique_mz <- numeric()
  for (iscan in seq(along = scan_indices)) {
    scans[[iscan]] <- xcms::getScan(raw_data, scan_indices[iscan], selection$mzrange)
    unique_mz <- unique(c(unique_mz, scans[[iscan]][, "mz"]))
  }
  
  unique_mz <- sort(unique_mz)
  
  list(mz = unique_mz, scans = scans)
}

average_scans_list <- function(scans_list){
  
  scans <- scans_list$scans
  unique_mz <- scans_list$mz
  scan_indices <- seq_along(scans)
  
  intensity_matrix <- matrix(nrow = length(unique_mz), ncol = length(scan_indices))
  
  for (iscan in scan_indices) {
    intensity_matrix[, iscan] <- approx(x = scans[[iscan]][, "mz"], y = scans[[iscan]][, "intensity"],
                                        xout = unique_mz)$y
  }
  
  data.frame(mz = unique_mz, intensity = rowMeans(intensity_matrix))
  
}
```

### Normalize Scans

Ideally, in our case, we also want to be able to normalize the scans before
we average them.

```{r normalize_scans}
normalize_scans_list <- function(scans_list, normalization_factors, scan_indices){
  scans <- scans_list$scans
  
  scans <- scans[scan_indices]
  
  normalized_scans <- purrr::map(seq_along(scan_indices), function(in_index){
    tmp_scan <- scans[[in_index]]
    
    tmp_scan[, "intensity"] <- exp(log(tmp_scan[, "intensity"]) - normalization_factors[in_index])
    
    tmp_scan
    
  })
  
  scans_list$scans <- normalized_scans
  scans_list
}
```

## Comparisons

Now that we have all of our functions defined, lets compare what happens when
we average raw and normalized data.

```{r load_processed_data}
zip_processed <- zip_ms("test_files/zip_files_profiling_X2017.11.15.16.04.25/bcal_good_UK001N1exoposb.zip")
raw_data <- zip_processed$raw_ms$raw_data
scan_range <- range(zip_processed$raw_ms$scan_range)

zip_processed$load_peak_finder()
normalization_factors <- zip_processed$peak_finder$correspondent_peaks$master_peak_list$normalization_factors
scan_indices <- zip_processed$peak_finder$correspondent_peaks$master_peak_list$scan_indices
```

```{r raw_data_averaging}
raw_scans <- extract_scans_list(raw_data, scan_range = scan_range)

# need to make the set of scans that get averaged the same
raw_scans$scans <- raw_scans$scans[scan_indices]

raw_average <- average_scans_list(raw_scans)
```

```{r average_normalized_data}
norm_scans <- extract_scans_list(raw_data, scan_range = scan_range)

norm_scans <- normalize_scans_list(norm_scans, normalization_factors, scan_indices)

norm_average <- average_scans_list(norm_scans)
```

## Compare Peaks & Distributions

```{r raw_peaks}
raw_average <- filter(raw_average, !is.na(intensity))
raw_peaks <- SIRM.FTMS.peakCharacterization:::generate_peaks(raw_average, n_peak = Inf)
```

```{r norm_peaks}
norm_average <- filter(norm_average, !is.na(intensity))
norm_peaks <- SIRM.FTMS.peakCharacterization:::generate_peaks(norm_average, n_peak = Inf)
```


```{r plot_them, fig.width=8, fig.height = 6}
raw_peaks$type <- "raw"
norm_peaks$type <- "normalized"

all_data <- rbind(raw_peaks, norm_peaks)
all_data$type <- forcats::fct_relevel(all_data$type, "raw", "normalized")

ggplot(all_data, aes(x = log10(Height))) + geom_histogram(bins = 200, position = "identity") + facet_wrap(~ type, nrow = 2, ncol = 1) + xlim(c(-.6, 6))
```

OK, this is interesting. We definitely broadened the distribution. I think this
will take a bit more investigation to figure out if the noise peaks are just in
the very smallest bit of the distribution, or the broad one ranging from 0 to 2.
