---
title: "Transient Level Peak Picking"
author: "Robert M Flight"
date: "`r Sys.time()`"
commit: "`r substr(git2r::branch_target(git2r::head(git2r::repository())), 1, 8)`"
output: 
  pdf_document: 
    toc: yes
---

# Purpose

Working out how to do transient level peak picking.

# Data

Lets load up some data working at the transient level.

```{r load_transient_level}
library(notifier)

library(xcms)
library(dplyr)
library(ggplot2)
library(SIRM.FTMS.peakPickingMethods)
library(ggplot2)
options(digits = 16)
```

So, our basic idea is to start very simply, and then escalate for peaks where the
simple things don't work. This requires us to set some simple metrics that tell
us whether or not a peak was likely picked correctly.

The steps I am imagining are:

* Classify as a flat or pointed peak
  * This is important because it changes how we handle looking at the center from
  the model.
* Fit a parabolic model to log-transformed values
* Flat peaks: check that center intensity is above max point, and between the flat points
* Pointed peaks: check that center intensity is within a weighted value of max, and near to it
* If checks fail, then:
* Try weighted parabolic fit, where weights are derived from point intensities
* Check again
* Try weighted **cauchy** fit
* Record generated parameters, and whether sanity checks pass for a peak.

## Peak Correspondence

We've got a bunch of methods for summarizing the peaks down to a single location
and intensity, now we need to figure out which one is best and most consistent.
To help with this, we need to match up the peaks across scans and get peak
correspondence. Lets play with that.

For peak correspondence, we need to set sensible limits for searching for a 
matching peak. To start, we will use a limit based on the digital resolution.

After an initial pass at matching peaks, we might use a calculation of the 
standard deviation of a window of m/z. To enable this, we take a middle m/z 
(should be the actual value of a correspondent peak), and go so many m/z on either
side (say 3 SD's), and then take **all** of the peaks from all of the scans within that window,
and calculate their squared distance to (the main peak / their own correspondent peak)
and then divide by (the number of peaks total - the number of corresponding peaks).

We may be able to find an interesting normalization factor by taking the most
intense average peak, and the ratio of its correspondent scan peaks to it,
and then the set of peaks that are within 1/2 of the intensity of that peak.


```{r raw_data, eval = FALSE}
raw_92C <- RawMS$new("92Cpos.mzML", "92Cpos_metadata.json")
```

```{r load_data}
load("raw_92C.RData")
```


```{r digital_resolution}
all_scans <- lapply(seq(raw_92C$scan_range), function(in_scan){
  tmp_rawdata <- as.data.frame(xcms::getScan(raw_92C$raw_data, in_scan))
  filter_rawdata <- SIRM.FTMS.peakPickingMethods:::get_scan_nozeros(tmp_rawdata)
  filter_rawdata
})

all_scans_merge <- do.call(rbind, all_scans)
all_fit <- exponential_fit(all_scans_merge$mz, all_scans_merge$lag, n_exp = 3)$coefficients
all_fit <- data.frame(coef = all_fit, which = letters[1:4], type = "all")

scan_models <- lapply(seq(raw_92C$scan_range), function(in_scan){
  tmp_rawdata <- as.data.frame(xcms::getScan(raw_92C$raw_data, in_scan))
  filter_rawdata <- SIRM.FTMS.peakPickingMethods:::get_scan_nozeros(tmp_rawdata)
  tmp <- exponential_fit(filter_rawdata$mz, filter_rawdata$lag, n_exp = 3)$coefficients
  data.frame(coef = tmp, which = letters[1:4], type = "indiv")
})

scan_coeffs <- do.call(rbind, scan_models)

mean_coeffs <- dplyr::group_by(scan_coeffs, which) %>% dplyr::summarise(., coef = mean(coef))

library(ggforce)

ggplot(scan_coeffs, aes(x = which, y = coef)) + geom_sina(bins = 40) +
  geom_point(data = all_fit, color = "blue") +
  geom_point(data = mean_coeffs, color = "red") + 
  facet_wrap(~which, scales = "free")
```

Based on this, the coefficients aren't changing a whole lot.

Lets see what happens when we graph the predictions from both against each other

```{r graph_preds}
mean_pred <- data.frame(x = rep(all_scans[[1]]$mz, 2),
                        y = c(exponential_predict(mean_coeffs$coef, all_scans[[1]]$mz),
                              exponential_predict(all_fit$coef, all_scans[[1]]$mz)),
                        which = rep(c("mean", "all"), each = nrow(all_scans[[1]])))

ggplot(mean_pred, aes(x = x, y = y, color = which)) + geom_point()

tmp_pred <- data.frame(mean = filter(mean_pred, which == "mean") %>% select(., y),
                       all = filter(mean_pred, which == "all") %>% select(., y))
names(tmp_pred) <- c("mean", "all")
ggplot(tmp_pred, aes(x = mean, y = all)) + geom_point()

tmp_pred <- mutate(tmp_pred, diff = abs(mean - all))
tmp_pred$mz <- all_scans[[1]]$mz

ggplot(tmp_pred, aes(x = diff)) + geom_histogram(bins = 40)
```



```{r get_peaks_from_multiple_scans, cache=TRUE, cache.lazy=FALSE}
options(mc.cores = 10)
multi_scan <- MultiScans$new(raw_92C)
```

Lets test various multipliers of the resolution to see where we get diminishing
returns on widening the resolution.

```{r correspondence, cache = TRUE, cache.lazy=FALSE}
thirds <- MasterPeakList$new(multi_scan, res_multiplier = 1/3)

seconds <- MasterPeakList$new(multi_scan, res_multiplier = 1/2)

once <- MasterPeakList$new(multi_scan, res_multiplier = 1)

twice2 <- MasterPeakList$new(multi_scan, res_multiplier = 2)

three <- MasterPeakList$new(multi_scan, res_multiplier = 3)
```

```{r all_novel}
all_novel <- list(thirds = data.frame(name = "thirds",
                                      n_peak = thirds$novel_peaks,
                                      scan = seq(1, length(thirds$novel_peaks))),
                  seconds = data.frame(name = "seconds",
                                      n_peak = seconds$novel_peaks,
                                      scan = seq(1, length(seconds$novel_peaks))),
                  once = data.frame(name = "once",
                                      n_peak = once$novel_peaks,
                                      scan = seq(1, length(once$novel_peaks))),
                  twice = data.frame(name = "twice",
                                      n_peak = twice$novel_peaks,
                                      scan = seq(1, length(twice$novel_peaks))),
                  three = data.frame(name = "three",
                                      n_peak = three$novel_peaks,
                                      scan = seq(1, length(three$novel_peaks))))
all_novel <- do.call(rbind, all_novel)
ggplot(all_novel, aes(x = scan, y = n_peak, color = name)) + geom_line()
```

It looks like from 2x to 3x there is very little difference in the number of novel
peaks as each scan is added. So let's use 2x as the default from here on out.



# Workflow for Transient-Level Peak Finding

* Transform to log-space
* Find peaks using `pracma::findpeaks`
* For each peak:
    * Check that non-zero points have significant area
    * Fit a parabolic model to non-zero points
    * Find the peak center and intensity based on the model
    * Integrate parabola and sides to get peak area


