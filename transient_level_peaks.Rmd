---
title: "Transient Level Peak Picking"
author: "Robert M Flight"
date: "`r Sys.time()`"
commit: "`r substr(git2r::branch_target(git2r::head(git2r::repository())), 1, 8)`"
output: 
  pdf_document: 
    toc: yes
---

# Purpose

Working out how to do transient level peak picking.

# Data

Lets load up some data working at the transient level.

```{r load_transient_level}
library(notifier)

library(xcms)
library(dplyr)
library(ggplot2)
library(SIRM.FTMS.peakPickingMethods)
library(ggplot2)
options(digits = 16)
```

So, our basic idea is to start very simply, and then escalate for peaks where the
simple things don't work. This requires us to set some simple metrics that tell
us whether or not a peak was likely picked correctly.

The steps I am imagining are:

* Classify as a flat or pointed peak
  * This is important because it changes how we handle looking at the center from
  the model.
* Fit a parabolic model to log-transformed values
* Flat peaks: check that center intensity is above max point, and between the flat points
* Pointed peaks: check that center intensity is within a weighted value of max, and near to it
* If checks fail, then:
* Try weighted parabolic fit, where weights are derived from point intensities
* Check again
* Try weighted **cauchy** fit
* Record generated parameters, and whether sanity checks pass for a peak.

## Peak Correspondence

We've got a bunch of methods for summarizing the peaks down to a single location
and intensity, now we need to figure out which one is best and most consistent.
To help with this, we need to match up the peaks across scans and get peak
correspondence. Lets play with that.

For peak correspondence, we need to set sensible limits for searching for a 
matching peak. To start, we will use a limit based on the digital resolution.

After an initial pass at matching peaks, we might use a calculation of the 
standard deviation of a window of m/z. To enable this, we take a middle m/z 
(should be the actual value of a correspondent peak), and go so many m/z on either
side, and then take **all** of the peaks from all of the scans within that window,
and calculate their squared distance to (the main peak / their own correspondent peak)
and then divide by (the number of peaks total - the number of corresponding peaks).


```{r raw_data, eval = FALSE}
raw_92C <- RawMS$new("92Cpos.mzML", "92Cpos_metadata.json")
```

```{r load_data}
load("raw_92C.RData")
```


```{r digital_resolution}
all_scans <- lapply(seq(raw_92C$scan_range), function(in_scan){
  tmp_rawdata <- as.data.frame(xcms::getScan(raw_92C$raw_data, in_scan))
  filter_rawdata <- SIRM.FTMS.peakPickingMethods:::get_scan_nozeros(tmp_rawdata)
  filter_rawdata
})

all_scans_merge <- do.call(rbind, all_scans)
all_fit <- exponential_fit(all_scans_merge$mz, all_scans_merge$lag, n_exp = 3)$coefficients
all_fit <- data.frame(coef = all_fit, which = letters[1:4], type = "all")

scan_models <- lapply(seq(raw_92C$scan_range), function(in_scan){
  tmp_rawdata <- as.data.frame(xcms::getScan(raw_92C$raw_data, in_scan))
  filter_rawdata <- SIRM.FTMS.peakPickingMethods:::get_scan_nozeros(tmp_rawdata)
  tmp <- exponential_fit(filter_rawdata$mz, filter_rawdata$lag, n_exp = 3)$coefficients
  data.frame(coef = tmp, which = letters[1:4], type = "indiv")
})

scan_coeffs <- do.call(rbind, scan_models)

mean_coeffs <- dplyr::group_by(scan_coeffs, which) %>% dplyr::summarise(., coef = mean(coef))

library(ggforce)

ggplot(scan_coeffs, aes(x = which, y = coef)) + geom_sina(bins = 40) +
  geom_point(data = all_fit, color = "blue") +
  geom_point(data = mean_coeffs, color = "red") + 
  facet_wrap(~which, scales = "free")
```

Based on this, the coefficients aren't changing a whole lot.

Lets see what happens when we graph the predictions from both against each other

```{r graph_preds}
mean_pred <- data.frame(x = rep(all_scans[[1]]$mz, 2),
                        y = c(exponential_predict(mean_coeffs$coef, all_scans[[1]]$mz),
                              exponential_predict(all_fit$coef, all_scans[[1]]$mz)),
                        which = rep(c("mean", "all"), each = nrow(all_scans[[1]])))

ggplot(mean_pred, aes(x = x, y = y, color = which)) + geom_point()

tmp_pred <- data.frame(mean = filter(mean_pred, which == "mean") %>% select(., y),
                       all = filter(mean_pred, which == "all") %>% select(., y))
names(tmp_pred) <- c("mean", "all")
ggplot(tmp_pred, aes(x = mean, y = all)) + geom_point()

tmp_pred <- mutate(tmp_pred, diff = abs(mean - all))
tmp_pred$mz <- all_scans[[1]]$mz

ggplot(tmp_pred, aes(x = diff)) + geom_histogram(bins = 40)
```



```{r get_peaks_from_multiple_scans, cache=TRUE, cache.lazy=FALSE}
options(mc.cores = 10)
multi_scan <- MultiScans$new(raw_92C)
```


```{r correspondence, cache = TRUE, cache.lazy=FALSE}
thirds <- MasterPeakList$new(multi_scan, res_multiplier = 1/3)

seconds <- MasterPeakList$new(multi_scan, res_multiplier = 1/2)

once <- MasterPeakList$new(multi_scan, res_multiplier = 1)

twice <- MasterPeakList$new(multi_scan, res_multiplier = 2)

three <- MasterPeakList$new(multi_scan, res_multiplier = 3)
```


# Workflow for Transient-Level Peak Finding

* Transform to log-space
* Find peaks using `pracma::findpeaks`
* For each peak:
    * Check that non-zero points have significant area
    * Fit a parabolic model to non-zero points
    * Find the peak center and intensity based on the model
    * Integrate parabola and sides to get peak area


